{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsw0W6v9DxCi"
      },
      "outputs": [],
      "source": [
        "# llama CSQA\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = []\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        max_next_q = max([self.get_q_value(next_state, a) for a in self.actions] or [0])\n",
        "        old_q = self.q_table.get((state, action), 0.0)\n",
        "        self.q_table[(state, action)] = old_q + self.alpha * (reward + self.gamma * max_next_q - old_q)\n",
        "\n",
        "class RDESelector:\n",
        "    def __init__(self, knowledge_base, model, tokenizer, pipeline, k=5):\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.k = k\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform([ex['question'] for ex in knowledge_base])\n",
        "        self.agent = QLearningAgent()\n",
        "        self.agent.actions = list(range(len(knowledge_base)))\n",
        "        self.pipe = pipeline\n",
        "\n",
        "    def _predict_answer(self, prompt):\n",
        "        \"\"\"Improved answer extraction with fallback\"\"\"\n",
        "        try:\n",
        "            output = self.pipe(\n",
        "                prompt,\n",
        "                max_new_tokens=50,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )[0]['generated_text']\n",
        "\n",
        "            match = re.search(r'\\b([A-E])\\b', output.upper())\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "            for choice in ['A', 'B', 'C', 'D', 'E']:\n",
        "                if f\" {choice} \" in output:\n",
        "                    return choice\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _get_reward(self, state_indices, action_idx, current_example):\n",
        "        selected_indices = state_indices + (action_idx,)\n",
        "        selected = [self.knowledge_base[i] for i in selected_indices[:self.k]]\n",
        "        prompt = self._create_prompt(selected, current_example)\n",
        "        prediction = self._predict_answer(prompt)\n",
        "        return 1 if prediction == current_example['answerKey'] else 0\n",
        "\n",
        "    def _create_prompt(self, demos, current_q=None):\n",
        "        messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Answer commonsense questions by selecting the correct option (A-E). First think step-by-step, then provide your final answer.\"\n",
        "        }]\n",
        "\n",
        "        for d in demos:\n",
        "            q_text = f\"{d['question']}\\nOptions:\\n\" + \"\\n\".join(\n",
        "                [f\"{label}: {text}\" for label, text in zip(d['choices']['label'], d['choices']['text'])]\n",
        "            )\n",
        "            messages.append({\"role\": \"user\", \"content\": q_text})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": f\"Answer: {d['answerKey']}\"})\n",
        "\n",
        "        if current_q:\n",
        "            target_q = f\"{current_q['question']}\\nOptions:\\n\" + \"\\n\".join(\n",
        "                [f\"{label}: {text}\" for label, text in zip(current_q['choices']['label'], current_q['choices']['text'])]\n",
        "            )\n",
        "            messages.append({\"role\": \"user\", \"content\": target_q})\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    def train(self, train_data, num_episodes=100):\n",
        "        progress_bar = tqdm(range(num_episodes), desc=\"Training\")\n",
        "        for episode in progress_bar:\n",
        "            total_reward = 0\n",
        "            for ex in train_data:\n",
        "                input_text = ex['question']\n",
        "                input_vec = self.vectorizer.transform([input_text])\n",
        "                similarities = cosine_similarity(input_vec, self.tfidf_matrix).flatten()\n",
        "                sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "                state = tuple(sorted_indices[:self.k])\n",
        "                action = self._choose_action(state)\n",
        "                reward = self._get_reward(state, action, ex)\n",
        "                next_state = tuple(sorted_indices[:self.k] + [action])[:self.k]\n",
        "                self.agent.update_q_value(state, action, reward, next_state)\n",
        "                total_reward += reward\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                \"Episode\": episode+1,\n",
        "                \"Avg Reward\": total_reward/len(train_data)\n",
        "            })\n",
        "\n",
        "    def _choose_action(self, state):\n",
        "        if np.random.random() < self.agent.epsilon:\n",
        "            return np.random.choice(self.agent.actions)\n",
        "        return max(self.agent.actions, key=lambda a: self.agent.get_q_value(state, a))\n",
        "\n",
        "    def select_demos(self, input_text):\n",
        "        input_vec = self.vectorizer.transform([input_text])\n",
        "        similarities = cosine_similarity(input_vec, self.tfidf_matrix).flatten()\n",
        "        sorted_indices = np.argsort(similarities)[::-1]\n",
        "        state = tuple(sorted_indices[:self.k])\n",
        "        action = self._choose_action(state)\n",
        "        selected_indices = state + (action,)\n",
        "        return [self.knowledge_base[i] for i in selected_indices][:self.k]\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.7,\n",
        "    device_map=\"auto\",\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "\n",
        "csqa = load_dataset('tau/commonsense_qa')\n",
        "print(\"Sample validation example:\", csqa['validation'][0])\n",
        "\n",
        "train_data = [{\n",
        "    'question': ex['question'],\n",
        "    'choices': ex['choices'],\n",
        "    'answerKey': ex['answerKey']\n",
        "} for ex in csqa['train']#.select(range(5000))]\n",
        "\n",
        "selector = RDESelector(train_data, model, tokenizer, pipe, k=5)\n",
        "\n",
        "selector.train(csqa['validation'].select(range(400)), num_episodes=7)\n",
        "\n",
        "def evaluate(selector, test_data):\n",
        "    correct = 0\n",
        "    progress_bar = tqdm(test_data, desc=\"Evaluating\")\n",
        "    for ex in progress_bar:\n",
        "        if 'answerKey' not in ex:\n",
        "            continue\n",
        "\n",
        "        demos = selector.select_demos(ex['question'])\n",
        "        prompt = selector._create_prompt(demos, ex)\n",
        "        prediction = selector._predict_answer(prompt)\n",
        "\n",
        "        if prediction and prediction == ex['answerKey']:\n",
        "            correct += 1\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            \"Current Accuracy\": f\"{correct/(progress_bar.n+1):.2f}\",\n",
        "            \"Prediction\": prediction or 'None',\n",
        "            \"Correct\": ex.get('answerKey', '?')\n",
        "        })\n",
        "\n",
        "    return correct / len(test_data) if test_data else 0\n",
        "\n",
        "\n",
        "def verify_answers(dataset_split):\n",
        "    return [ex for ex in dataset_split if ex['answerKey'] in ['A','B','C','D','E']]\n",
        "\n",
        "test_samples = csqa['validation'] #.select(range(450, 1200))\n",
        "\n",
        "test_samples = verify_answers(csqa['validation'] #.select(range(450, 1200)))\n",
        "accuracy = evaluate(selector, test_samples)\n",
        "print(f\"Final Test Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT2 CSQA\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha: float = 0.1, gamma: float = 0.9, epsilon: float = 0.1):\n",
        "        self.q_table = {}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.valid_actions_map = {}\n",
        "\n",
        "    def get_action(self, state: str, valid_actions: List[int]) -> int:\n",
        "        if np.random.random() < self.epsilon or state not in self.q_table:\n",
        "            return int(np.random.choice(valid_actions))\n",
        "        return max(self.q_table[state], key=self.q_table[state].get)\n",
        "\n",
        "    def update_q_table(self, state: str, action: int, reward: float,\n",
        "                      next_state: str, valid_actions: List[int]):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {a: 0.0 for a in valid_actions}\n",
        "            self.valid_actions_map[state] = valid_actions\n",
        "\n",
        "        old_value = self.q_table[state].get(action, 0)\n",
        "        next_max = max(self.q_table[next_state].values()) if next_state in self.q_table else 0\n",
        "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
        "        self.q_table[state][action] = new_value\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, knowledge_base: List[Dict]):\n",
        "        self.kb = knowledge_base\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def calculate_diversity(self, selected_demos: List[Dict]) -> float:\n",
        "        answers = [demo['answerKey'] for demo in selected_demos]\n",
        "        return len(set(answers)) / len(answers) if len(selected_demos) > 0 else 0.0\n",
        "\n",
        "    def get_reward(self, input_text: Dict, selected_demos: List[Dict]) -> float:\n",
        "        prompt = self._construct_prompt(input_text, selected_demos)\n",
        "        prediction = self._llm_predictor(prompt, input_text)\n",
        "        return 1.0 if prediction == input_text['answerKey'] else 0.0\n",
        "\n",
        "    def _construct_prompt(self, input_text: Dict, demos: List[Dict]) -> str:\n",
        "        prompt = \"\"\n",
        "        for demo in demos:\n",
        "            prompt += f\"Q: {demo['question']}\\n\"\n",
        "            prompt += \"\\n\".join([f\"{k}: {v}\" for k, v in demo['choices'].items()])\n",
        "            prompt += f\"\\nA: {demo['answerKey']}\\n\\n\"\n",
        "        prompt += f\"Q: {input_text['question']}\\n\"\n",
        "        prompt += \"\\n\".join([f\"{k}: {v}\" for k, v in input_text['choices'].items()])\n",
        "        prompt += \"\\nA:\"\n",
        "        return prompt\n",
        "\n",
        "    def _llm_predictor(self, prompt: str, input_text: Dict) -> str:\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=1,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True\n",
        "        )\n",
        "\n",
        "        answer_token = self.tokenizer.decode(outputs.sequences[0, -1])\n",
        "        return answer_token.strip()\n",
        "\n",
        "class RDESEvaluator:\n",
        "    def __init__(self, agent: QLearningAgent, env: Environment):\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "\n",
        "    def select_demonstrations(self, input_text: Dict, k: int) -> Tuple[List[Dict], float]:\n",
        "        texts = [d['question'] for d in self.env.kb] + [input_text['question']]\n",
        "        tfidf_matrix = self.env.vectorizer.fit_transform(texts)\n",
        "\n",
        "        input_vec = tfidf_matrix[-1]\n",
        "        demo_vecs = tfidf_matrix[:-1]\n",
        "        similarities = cosine_similarity(input_vec, demo_vecs)[0]\n",
        "\n",
        "        top_indices = np.argsort(similarities)[-k:][::-1]\n",
        "        selected = [self.env.kb[int(i)] for i in top_indices]\n",
        "\n",
        "        diversity = self.env.calculate_diversity(selected)\n",
        "        if diversity < 0.6:\n",
        "            remaining_indices = np.argsort(similarities)[:-k]\n",
        "            for idx in remaining_indices[::-1]:\n",
        "                if len(selected) >= k:\n",
        "                    break\n",
        "                selected.append(self.env.kb[int(idx)])\n",
        "                diversity = self.env.calculate_diversity(selected)\n",
        "                if diversity >= 0.6:\n",
        "                    break\n",
        "\n",
        "        return selected, diversity\n",
        "\n",
        "    def train(self, episodes: int, demo_set_size: int = 5):\n",
        "        self.env.model.train()\n",
        "        for episode in range(episodes):\n",
        "            state = str(np.random.choice(list(self.agent.q_table.keys()))) if self.agent.q_table else \"init\"\n",
        "            for _ in range(demo_set_size):\n",
        "                valid_actions = list(range(len(self.env.kb)))\n",
        "                action = self.agent.get_action(state, valid_actions)\n",
        "\n",
        "                next_state = f\"state_{action}\"\n",
        "                reward = self.env.get_reward(self.env.kb[int(action)], [])\n",
        "\n",
        "                self.agent.update_q_table(state, action, reward, next_state, valid_actions)\n",
        "                state = next_state\n",
        "        self.env.model.eval()\n",
        "\n",
        "    def evaluate(self, test_set: List[Dict], demo_set_size: int = 5) -> float:\n",
        "        total, correct = 0, 0\n",
        "        for example in test_set:\n",
        "            selected_demos, _ = self.select_demonstrations(example, demo_set_size)\n",
        "            reward = self.env.get_reward(example, selected_demos)\n",
        "            correct += reward\n",
        "            total += 1\n",
        "        return correct / total\n",
        "\n",
        "dataset = load_dataset(\"tau/commonsense_qa\")\n",
        "train_data = dataset['train'].shuffle(seed=42)#.select(range(100))\n",
        "print(\"train data size: \", train_data.shape)\n",
        "test_data = dataset['validation'].shuffle(seed=42)#.select(range(50))\n",
        "print(\"test data size: \", test_data.shape)\n",
        "\n",
        "agent = QLearningAgent()\n",
        "env = Environment(train_data)\n",
        "evaluator = RDESEvaluator(agent, env)\n",
        "\n",
        "evaluator.train(episodes=7)\n",
        "accuracy = evaluator.evaluate(test_data)\n",
        "print(f\"RDES Evaluation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Fadu1UocD0xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemma CSQA\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class RDESelector:\n",
        "    def __init__(self, demo_pool, num_classes, q_table=None):\n",
        "        self.demo_pool = demo_pool\n",
        "        self.num_classes = num_classes\n",
        "        self.q_table = q_table if q_table else {}\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.demo_embeddings = self.embedding_model.encode(\n",
        "            [d['question'] for d in demo_pool],\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "    def diversity_score(self, selected_indices):\n",
        "        answer_counts = np.zeros(self.num_classes)\n",
        "        for idx in selected_indices:\n",
        "            answer_idx = ord(self.demo_pool[idx]['answerKey']) - ord('A')\n",
        "            answer_counts[answer_idx] += 1\n",
        "        entropy = -np.sum((answer_counts/np.sum(answer_counts)) *\n",
        "                        np.log(answer_counts/np.sum(answer_counts) + 1e-9))\n",
        "        return entropy\n",
        "\n",
        "    def get_state_key(self, current_state):\n",
        "        return tuple(sorted(current_state['selected']))\n",
        "\n",
        "    def select_demos(self, input_sample, k=5):\n",
        "        selected = []\n",
        "        state = {'input': input_sample, 'selected': []}\n",
        "\n",
        "        input_embedding = self.embedding_model.encode(\n",
        "            input_sample['question'],\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        for _ in range(k):\n",
        "            valid_demos = [i for i in range(len(self.demo_pool))\n",
        "                         if i not in state['selected']]\n",
        "\n",
        "            if np.random.random() < self.epsilon:\n",
        "                action = int(np.random.choice(valid_demos))\n",
        "            else:\n",
        "                q_values = [self.q_table.get((self.get_state_key(state), a), 0)\n",
        "                          for a in valid_demos]\n",
        "                action = int(valid_demos[np.argmax(q_values)])\n",
        "\n",
        "            selected.append(action)\n",
        "            state['selected'].append(action)\n",
        "\n",
        "            next_state = state.copy()\n",
        "            next_state['selected'] = selected.copy()\n",
        "            reward = self.calculate_reward(input_embedding, selected)\n",
        "\n",
        "            current_state_key = self.get_state_key(state)\n",
        "            next_max = max([self.q_table.get((self.get_state_key(next_state), a), 0)\n",
        "                          for a in valid_demos if a != action], default=0)\n",
        "\n",
        "            self.q_table[(current_state_key, action)] = (\n",
        "                (1 - self.alpha) * self.q_table.get((current_state_key, action), 0) +\n",
        "                self.alpha * (reward + self.gamma * next_max)\n",
        "            )\n",
        "\n",
        "        return [self.demo_pool[i] for i in selected]\n",
        "\n",
        "    def calculate_reward(self, input_embedding, selected_indices):\n",
        "        demo_embeddings = self.demo_embeddings[selected_indices]\n",
        "        similarities = np.dot(demo_embeddings, input_embedding)\n",
        "        relevance = np.mean(similarities)\n",
        "\n",
        "        diversity = self.diversity_score(selected_indices)\n",
        "        max_entropy = np.log(self.num_classes)\n",
        "        normalized_diversity = diversity / max_entropy\n",
        "\n",
        "        return 0.5 * normalized_diversity + 0.5 * relevance\n",
        "\n",
        "commonsense_qa = load_dataset(\"tau/commonsense_qa\")\n",
        "train_demos = commonsense_qa[\"train\"]#.select(range(1000))\n",
        "test_samples = commonsense_qa[\"validation\"]#.select(range(100))\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"low_cpu_mem_usage\": True}\n",
        ")\n",
        "pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id\n",
        "\n",
        "\n",
        "\n",
        "rde_selector = RDESelector(train_demos, num_classes=5)\n",
        "\n",
        "def format_prompt(demos, test_sample):\n",
        "    prompt = \"Answer these commonsense questions. Choose the best option from A-E.\\n\\n\"\n",
        "    for demo in demos:\n",
        "        prompt += f\"Q: {demo['question']}\\nOptions:\\n\"\n",
        "        for label, text in zip(demo['choices']['label'], demo['choices']['text']):\n",
        "            prompt += f\"{label}: {text}\\n\"\n",
        "        prompt += f\"Answer: {demo['answerKey']}\\n\\n\"\n",
        "\n",
        "    prompt += f\"Q: {test_sample['question']}\\nOptions:\\n\"\n",
        "    for label, text in zip(test_sample['choices']['label'], test_sample['choices']['text']):\n",
        "        prompt += f\"{label}: {text}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "    return prompt\n",
        "\n",
        "correct = 0\n",
        "for idx, sample in enumerate(test_samples):\n",
        "    selected_demos = rde_selector.select_demos(sample, k=5)\n",
        "    prompt = format_prompt(selected_demos, sample)\n",
        "\n",
        "    outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=2,\n",
        "        return_full_text=False\n",
        "    )\n",
        "\n",
        "    predicted = outputs[0]['generated_text'].strip()[0]\n",
        "    actual = sample['answerKey']\n",
        "\n",
        "    if predicted.upper() == actual:\n",
        "        correct += 1\n",
        "\n",
        "    color_code = \"\\033[92m\" if str(actual) == str(predicted) else \"\\033[91m\"\n",
        "    print(f\"Sample {idx+1}:\")\n",
        "    print(f\"  Predicted: {predicted} | Actual: {actual}\")\n",
        "    # print(f\"  Text: {sample['text'][:100]}...\")\n",
        "    print(f\"{color_code}  Result: {'CORRECT' if str(actual) == str(predicted) else 'INCORRECT'}\\033[0m\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nFinal Accuracy: {correct/len(test_samples):.2%}\")\n"
      ],
      "metadata": {
        "id": "1W1-rIVDD0lw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}