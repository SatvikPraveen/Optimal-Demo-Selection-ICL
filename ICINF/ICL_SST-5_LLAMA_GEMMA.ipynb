{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbc7c2f-77b2-49da-8068-e8b0bdce2ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c069a27ed46d4fc88aeb4e144143269e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Evaluating subsets: 100%|██████████| 100/100 [2:27:40<00:00, 88.61s/it] \n",
      "Testing: 100%|██████████| 1000/1000 [01:25<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 34.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# SST-5 template from paper\n",
    "SST5_TEMPLATE = \"Text: {text}\\nSentiment: {label}\\n\\n\"\n",
    "\n",
    "def format_example(example):\n",
    "    return SST5_TEMPLATE.format(\n",
    "        text=example[\"text\"],\n",
    "        label=example[\"label\"]\n",
    "    )\n",
    "\n",
    "def get_label_probs(prompt, text):\n",
    "    full_input = prompt + f\"Text: {text}\\nSentiment:\"\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    \n",
    "    # Label tokens (adjust based on tokenizer's mapping)\n",
    "    label_tokens = [tokenizer(f\" {i}\")[\"input_ids\"][-1] for i in range(5)]\n",
    "    last_token_logits = outputs[0, -1, label_tokens]\n",
    "    return torch.softmax(last_token_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "def evaluate_subset(Si, val_set):\n",
    "    prompt = \"\".join([format_example(ex) for ex in Si])\n",
    "    correct = 0\n",
    "    for ex in val_set:\n",
    "        probs = get_label_probs(prompt, ex[\"text\"])\n",
    "        if np.argmax(probs) == ex[\"label\"]:\n",
    "            correct += 1\n",
    "    return correct / len(val_set)\n",
    "\n",
    "def compute_influences(train_set, val_set, k=5, M=100):\n",
    "    # Balanced subset sampling\n",
    "    class_counts = defaultdict(int)\n",
    "    for ex in train_set:\n",
    "        class_counts[ex[\"label\"]] += 1\n",
    "    \n",
    "    k_per_class = max(1, k // len(class_counts))\n",
    "    subsets = []\n",
    "    for _ in range(M):\n",
    "        Si = []\n",
    "        for label in class_counts:\n",
    "            candidates = [ex for ex in train_set if ex[\"label\"] == label]\n",
    "            Si.extend(np.random.choice(candidates, k_per_class, replace=False))\n",
    "        Si = list(np.random.choice(Si, k, replace=False))\n",
    "        subsets.append(Si)\n",
    "    \n",
    "    # Sequential evaluation\n",
    "    D = []\n",
    "    for Si in tqdm(subsets, desc=\"Evaluating subsets\"):\n",
    "        D.append(evaluate_subset(Si, val_set))\n",
    "    \n",
    "    # Calculate influences\n",
    "    influence_scores = defaultdict(list)\n",
    "    for idx, ex in enumerate(train_set):\n",
    "        included = []\n",
    "        excluded = []\n",
    "        for Si, acc in zip(subsets, D):\n",
    "            if ex in Si:\n",
    "                included.append(acc)\n",
    "            else:\n",
    "                excluded.append(acc)\n",
    "        \n",
    "        Nj = len(included)\n",
    "        Mj = len(D) - Nj\n",
    "        if Nj > 0 and Mj > 0:\n",
    "            influence = (sum(included)/Nj) - (sum(excluded)/Mj)\n",
    "        else:\n",
    "            influence = 0\n",
    "        influence_scores[idx] = influence\n",
    "    \n",
    "    return influence_scores\n",
    "\n",
    "def load_sst5(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"SetFit/sst5\", \"default\", split=split)\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "    return dataset\n",
    "\n",
    "def run_experiment():\n",
    "    # Load SST-5\n",
    "    train_set = load_sst5(\"train\", 5000)\n",
    "    val_set = load_sst5(\"validation\", 1000)\n",
    "    test_set = load_sst5(\"test\", 1000)\n",
    "    \n",
    "    # Compute influences\n",
    "    influence_scores = compute_influences(train_set, val_set, k=5, M=100)\n",
    "    \n",
    "    # Select top examples\n",
    "    sorted_indices = sorted(influence_scores, key=influence_scores.get, reverse=True)[:5]\n",
    "    top_examples = [train_set[i] for i in sorted_indices]\n",
    "    \n",
    "    # Final evaluation\n",
    "    prompt = \"\".join([format_example(ex) for ex in top_examples])\n",
    "    correct = 0\n",
    "    for ex in tqdm(test_set, desc=\"Testing\"):\n",
    "        probs = get_label_probs(prompt, ex[\"text\"])\n",
    "        if np.argmax(probs) == ex[\"label\"]:\n",
    "            correct += 1\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {correct/len(test_set):.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7dec5b6-4892-4c3f-a9d9-bc8426de7909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd83a6bbac9c428288710120b5d46ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Evaluating subsets: 100%|██████████| 100/100 [2:42:43<00:00, 97.63s/it]  \n",
      "Testing: 100%|██████████| 1000/1000 [01:40<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 51.10%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# SST-5 template from paper\n",
    "SST5_TEMPLATE = \"Text: {text}\\nSentiment: {label}\\n\\n\"\n",
    "\n",
    "def format_example(example):\n",
    "    return SST5_TEMPLATE.format(\n",
    "        text=example[\"text\"],\n",
    "        label=example[\"label\"]\n",
    "    )\n",
    "\n",
    "def get_label_probs(prompt, text):\n",
    "    full_input = prompt + f\"Text: {text}\\nSentiment:\"\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    \n",
    "    # Label tokens (adjust based on tokenizer's mapping)\n",
    "    label_tokens = [tokenizer(f\" {i}\")[\"input_ids\"][-1] for i in range(5)]\n",
    "    last_token_logits = outputs[0, -1, label_tokens]\n",
    "    return torch.softmax(last_token_logits, dim=-1).cpu().numpy()\n",
    "\n",
    "def evaluate_subset(Si, val_set):\n",
    "    prompt = \"\".join([format_example(ex) for ex in Si])\n",
    "    correct = 0\n",
    "    for ex in val_set:\n",
    "        probs = get_label_probs(prompt, ex[\"text\"])\n",
    "        if np.argmax(probs) == ex[\"label\"]:\n",
    "            correct += 1\n",
    "    return correct / len(val_set)\n",
    "\n",
    "def compute_influences(train_set, val_set, k=5, M=100):\n",
    "    # Balanced subset sampling\n",
    "    class_counts = defaultdict(int)\n",
    "    for ex in train_set:\n",
    "        class_counts[ex[\"label\"]] += 1\n",
    "    \n",
    "    k_per_class = max(1, k // len(class_counts))\n",
    "    subsets = []\n",
    "    for _ in range(M):\n",
    "        Si = []\n",
    "        for label in class_counts:\n",
    "            candidates = [ex for ex in train_set if ex[\"label\"] == label]\n",
    "            Si.extend(np.random.choice(candidates, k_per_class, replace=False))\n",
    "        Si = list(np.random.choice(Si, k, replace=False))\n",
    "        subsets.append(Si)\n",
    "    \n",
    "    # Sequential evaluation\n",
    "    D = []\n",
    "    for Si in tqdm(subsets, desc=\"Evaluating subsets\"):\n",
    "        D.append(evaluate_subset(Si, val_set))\n",
    "    \n",
    "    # Calculate influences\n",
    "    influence_scores = defaultdict(list)\n",
    "    for idx, ex in enumerate(train_set):\n",
    "        included = []\n",
    "        excluded = []\n",
    "        for Si, acc in zip(subsets, D):\n",
    "            if ex in Si:\n",
    "                included.append(acc)\n",
    "            else:\n",
    "                excluded.append(acc)\n",
    "        \n",
    "        Nj = len(included)\n",
    "        Mj = len(D) - Nj\n",
    "        if Nj > 0 and Mj > 0:\n",
    "            influence = (sum(included)/Nj) - (sum(excluded)/Mj)\n",
    "        else:\n",
    "            influence = 0\n",
    "        influence_scores[idx] = influence\n",
    "    \n",
    "    return influence_scores\n",
    "\n",
    "def load_sst5(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"SetFit/sst5\", \"default\", split=split)\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "    return dataset\n",
    "\n",
    "def run_experiment():\n",
    "    # Load SST-5\n",
    "    train_set = load_sst5(\"train\", 5000)\n",
    "    val_set = load_sst5(\"validation\", 1000)\n",
    "    test_set = load_sst5(\"test\", 1000)\n",
    "    \n",
    "    # Compute influences\n",
    "    influence_scores = compute_influences(train_set, val_set, k=5, M=100)\n",
    "    \n",
    "    # Select top examples\n",
    "    sorted_indices = sorted(influence_scores, key=influence_scores.get, reverse=True)[:5]\n",
    "    top_examples = [train_set[i] for i in sorted_indices]\n",
    "    \n",
    "    # Final evaluation\n",
    "    prompt = \"\".join([format_example(ex) for ex in top_examples])\n",
    "    correct = 0\n",
    "    for ex in tqdm(test_set, desc=\"Testing\"):\n",
    "        probs = get_label_probs(prompt, ex[\"text\"])\n",
    "        if np.argmax(probs) == ex[\"label\"]:\n",
    "            correct += 1\n",
    "    \n",
    "    print(f\"Final Test Accuracy: {correct/len(test_set):.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9cc88-55a4-4a58-9c39-2b055663946e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
