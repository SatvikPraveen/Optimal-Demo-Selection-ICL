{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7aaede9a-092a-4877-a103-0304f308b77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading and embedding SST-5 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422c882b7f24452085ca742b445095c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Starting batch evaluation (3 random queries)...\n",
      "\n",
      " Generated label: neutral\n",
      "\n",
      " Actual label: neutral\n",
      "\n",
      " Generated label: neutral\n",
      "\n",
      " Actual label: negative\n",
      "\n",
      " Generated label: very positive\n",
      "\n",
      " Actual label: positive\n",
      "\n",
      " Generated label: neutral\n",
      "\n",
      " Actual label: very negative\n",
      "\n",
      " Generated label: positive\n",
      "\n",
      " Actual label: negative\n",
      "\n",
      " Generated label: positive\n",
      "\n",
      " Actual label: negative\n",
      "\n",
      " Generated label: very negative\n",
      "\n",
      " Actual label: positive\n",
      "\n",
      " Generated label: neutral\n",
      "\n",
      " Actual label: neutral\n",
      "\n",
      " Generated label: neutral\n",
      "\n",
      " Actual label: negative\n",
      "\n",
      " Generated label: very negative\n",
      "\n",
      " Actual label: very positive\n",
      "\n",
      "[2] Evaluation over 10 samples\n",
      "Average Accuracy: 0.2000\n",
      "Average F1 Score: 0.2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "class TopK_CoNE:\n",
    "    def __init__(self, embeddings, raw_texts, k=5, model_name='gpt2', device=None):\n",
    "        self.embeddings = embeddings\n",
    "        self.raw_texts = raw_texts\n",
    "        self.k = k\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_topk(self, query_embedding):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "        topk_indices = np.argsort(similarities)[0:30][::-1]  #get top 30\n",
    "        return topk_indices\n",
    "\n",
    "    def compute_cross_entropy(self, text):\n",
    "        encodings = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        input_ids = encodings['input_ids']\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "        return loss * input_ids.size(1)\n",
    "\n",
    "    def apply_cone(self, query_text, topk_indices):\n",
    "        candidate_scores = []\n",
    "        for idx in topk_indices:\n",
    "            demo = self.raw_texts[idx]\n",
    "            prompt_with_query = demo + \"\\n\" + query_text\n",
    "            H_xc = self.compute_cross_entropy(prompt_with_query)\n",
    "            H_c = self.compute_cross_entropy(demo)\n",
    "            H_cond = H_xc - H_c\n",
    "            candidate_scores.append((idx, H_cond))\n",
    "        sorted_indices = [idx for idx, _ in sorted(candidate_scores, key=lambda x: x[1])]\n",
    "        return sorted_indices[:self.k]\n",
    "\n",
    "    def select_demonstrations(self, query_embedding, query_text):\n",
    "        topk_indices = self.get_topk(query_embedding)\n",
    "        refined_indices = self.apply_cone(query_text, topk_indices)\n",
    "        return refined_indices\n",
    "\n",
    "# ----------------------\n",
    "# SST-5 Dataset loading and embedding\n",
    "# ----------------------\n",
    "def load_sst5(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"SetFit/sst5\", \"default\", split=split)\n",
    "    dataset = dataset.filter(lambda x: x[\"label_text\"] is not None)\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        label_text = sample[\"label_text\"]\n",
    "        full_text = f\"Text: {text}\\nLabel: {label_text}\"\n",
    "        texts.append(full_text)\n",
    "        labels.append(label_text)\n",
    "    return texts, labels\n",
    "\n",
    "def embed_texts(texts, model_name='all-MiniLM-L6-v2', batch_size=64):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def generate_label_from_prompt(prompt, model, tokenizer, device):\n",
    "    enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=3,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    #print(\"\\n Prompt:\", prompt)\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    #print(\"\\n Output:\", decoded)\n",
    "    generated_label = decoded.split(\"Label:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "    print(\"\\n Generated label:\", generated_label)\n",
    "    return generated_label\n",
    "\n",
    "# ----------------------\n",
    "# Evaluate TopK + CoNE in batches of 3\n",
    "# ----------------------\n",
    "def evaluate_cone(texts, labels, embeddings, k=5, batch_size=3):\n",
    "    print(\"\\n[1] Starting batch evaluation (3 random queries)...\")\n",
    "    indices = random.sample(range(len(texts)), batch_size)\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    for idx in indices:\n",
    "        query_input = texts[idx].rsplit(\"\\nLabel:\", 1)[0] + \"\\nLabel:\"\n",
    "        query_label = labels[idx]\n",
    "        query_embedding = embeddings[idx]\n",
    "\n",
    "        candidate_embeddings = np.delete(embeddings, idx, axis=0)\n",
    "        candidate_texts = [t for i, t in enumerate(texts) if i != idx]\n",
    "        candidate_labels = [l for i, l in enumerate(labels) if i != idx]\n",
    "\n",
    "        selector = TopK_CoNE(candidate_embeddings, candidate_texts, k=k)\n",
    "        demo_indices = selector.select_demonstrations(query_embedding, query_input)\n",
    "\n",
    "        # Add labels to the demonstration text\n",
    "        demos = []\n",
    "        for i in demo_indices:\n",
    "            labeled_demo = candidate_texts[i] + f\"\\nLabel: {candidate_labels[i]}\"\n",
    "            demos.append(labeled_demo)\n",
    "\n",
    "        prompt = \"\\n\\n\".join(demos) + \"\\n\\n\" + query_input\n",
    "        pred_label = generate_label_from_prompt(prompt, selector.model, selector.tokenizer, selector.device)\n",
    "        \n",
    "        #print(\"pred label:\", pred_label)\n",
    "        print(\"\\n Actual label:\", query_label)\n",
    "        \n",
    "        accs.append(int(pred_label.lower() == query_label.lower()))\n",
    "        f1s.append(f1_score([query_label], [pred_label], average='macro'))\n",
    "\n",
    "    avg_acc = np.mean(accs)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    print(f\"\\n[2] Evaluation over {batch_size} samples\")\n",
    "    print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "# ----------------------\n",
    "# Main Driver\n",
    "# ----------------------\n",
    "def run_demo_selection():\n",
    "    print(\"\\n[1] Loading and embedding SST-5 dataset...\")\n",
    "    texts, labels = load_sst5(num_samples=100)\n",
    "    embeddings = embed_texts(texts)\n",
    "    evaluate_cone(texts, labels, embeddings, k=4, batch_size=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo_selection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5141f8b6-7601-41a7-9109-bc8e6796f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading and embedding CommonsenseQA dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5ccf809b66489391b5564c1ce88364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Starting batch evaluation (3 random queries)...\n",
      "pred label: all buildings\n",
      "\n",
      " Actual label: restaurant\n",
      "pred label: composted\n",
      "\n",
      " Actual label: composted\n",
      "pred label: satellite\n",
      "\n",
      " Actual label: beast\n",
      "\n",
      "[2] Evaluation over 3 samples\n",
      "Average Accuracy: 0.3333\n",
      "Average F1 Score: 0.3333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "class TopK_CoNE:\n",
    "    def __init__(self, embeddings, raw_texts, k=5, model_name='gpt2', device=None):\n",
    "        self.embeddings = embeddings\n",
    "        self.raw_texts = raw_texts\n",
    "        self.k = k\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_topk(self, query_embedding):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "        topk_indices = np.argsort(similarities)[-self.k:][::-1]\n",
    "        return topk_indices\n",
    "\n",
    "    def compute_cross_entropy(self, text):\n",
    "        encodings = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        input_ids = encodings['input_ids']\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "        return loss * input_ids.size(1)\n",
    "\n",
    "    def apply_cone(self, query_text, topk_indices):\n",
    "        candidate_scores = []\n",
    "        for idx in topk_indices:\n",
    "            demo = self.raw_texts[idx]\n",
    "            prompt_with_query = demo + \"\\n\" + query_text\n",
    "            H_xc = self.compute_cross_entropy(prompt_with_query)\n",
    "            H_c = self.compute_cross_entropy(demo)\n",
    "            H_cond = H_xc - H_c\n",
    "            candidate_scores.append((idx, H_cond))\n",
    "        sorted_indices = [idx for idx, _ in sorted(candidate_scores, key=lambda x: x[1])]\n",
    "        return sorted_indices[:self.k]\n",
    "\n",
    "    def select_demonstrations(self, query_embedding, query_text):\n",
    "        topk_indices = self.get_topk(query_embedding)\n",
    "        refined_indices = self.apply_cone(query_text, topk_indices)\n",
    "        return refined_indices\n",
    "\n",
    "# ----------------------\n",
    "# CommonsenseQA Dataset loading and embedding\n",
    "# ----------------------\n",
    "def load_commonsenseqa(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"commonsense_qa\", split=split)\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        question = example['question']\n",
    "        choices = example['choices']['text']\n",
    "        label_index = example['answerKey']\n",
    "        answer = example['choices']['label'].index(label_index)\n",
    "        \n",
    "        # Format choices and include them in the prompt\n",
    "        choices_text = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(example['choices']['label'], choices)])\n",
    "        prompt = f\"Question: {question}\\n{choices_text}\"\n",
    "        label_text = example['choices']['text'][answer]\n",
    "        \n",
    "        full_prompt = f\"{prompt}\\nAnswer: {label_text}\"\n",
    "        texts.append(full_prompt)\n",
    "        labels.append(label_text)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def embed_texts(texts, model_name='all-MiniLM-L6-v2', batch_size=64):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def generate_label_from_prompt(prompt, model, tokenizer, device):\n",
    "    enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_label = decoded.split(\"Answer:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "    return generated_label\n",
    "\n",
    "# ----------------------\n",
    "# Evaluate TopK + CoNE in batches of 3\n",
    "# ----------------------\n",
    "def evaluate_cone(texts, labels, embeddings, k=5, batch_size=3):\n",
    "    print(\"\\n[1] Starting batch evaluation (3 random queries)...\")\n",
    "    indices = random.sample(range(len(texts)), batch_size)\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    for idx in indices:\n",
    "        query_input = texts[idx].rsplit(\"\\nAnswer:\", 1)[0] + \"\\nAnswer:\"\n",
    "        query_label = labels[idx]\n",
    "        query_embedding = embeddings[idx]\n",
    "\n",
    "        candidate_embeddings = np.delete(embeddings, idx, axis=0)\n",
    "        candidate_texts = [t for i, t in enumerate(texts) if i != idx]\n",
    "        candidate_labels = [l for i, l in enumerate(labels) if i != idx]\n",
    "\n",
    "        selector = TopK_CoNE(candidate_embeddings, candidate_texts, k=k)\n",
    "        demo_indices = selector.select_demonstrations(query_embedding, query_input)\n",
    "\n",
    "        # Add labeled demos\n",
    "        demos = []\n",
    "        for i in demo_indices:\n",
    "            labeled_demo = candidate_texts[i] + f\"\\nAnswer: {candidate_labels[i]}\"\n",
    "            demos.append(labeled_demo)\n",
    "\n",
    "        prompt = \"\\n\\n\".join(demos) + \"\\n\\n\" + query_input\n",
    "        pred_label = generate_label_from_prompt(prompt, selector.model, selector.tokenizer, selector.device)\n",
    "        print(\"pred label:\", pred_label)\n",
    "        print(\"\\n Actual label:\", query_label)\n",
    "                \n",
    "        accs.append(int(pred_label.strip().lower() == query_label.strip().lower()))\n",
    "        f1s.append(f1_score([query_label], [pred_label], average='macro'))\n",
    "\n",
    "    avg_acc = np.mean(accs)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    print(f\"\\n[2] Evaluation over {batch_size} samples\")\n",
    "    print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "# ----------------------\n",
    "# Main Driver\n",
    "# ----------------------\n",
    "def run_demo_selection():\n",
    "    print(\"\\n[1] Loading and embedding CommonsenseQA dataset...\")\n",
    "    texts, labels = load_commonsenseqa(num_samples=100)\n",
    "    embeddings = embed_texts(texts)\n",
    "    evaluate_cone(texts, labels, embeddings, k=5, batch_size=3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo_selection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309f006-bd7a-4c5f-a4a8-dbb42eb88a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup \n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1e1de-0214-4f93-93d3-3b35833f5e89",
   "metadata": {},
   "source": [
    "Usage:\n",
    "First, pass a HF token to use gated models.\n",
    "\n",
    "At the bottom of the next block, put in a model name and a HF dataset of your choice.\n",
    "\n",
    "    Supported datasets:\n",
    "    - Commonsenseqa\n",
    "    - AGNews\n",
    "    - SST-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8278a-bfed-442a-b1e2-35159d69e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "class TopK_CoNE:\n",
    "    def __init__(self, embeddings, raw_texts, k=5):\n",
    "        self.embeddings = embeddings\n",
    "        self.raw_texts = raw_texts\n",
    "        self.k = k\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = None\n",
    "\n",
    "    def get_topk(self, query_embedding):\n",
    "        similarities = cosine_similarity(query_embedding.reshape(1, -1), self.embeddings)[0]\n",
    "        topk_indices = np.argsort(similarities)[-self.k:][::-1]\n",
    "        return topk_indices\n",
    "\n",
    "    def compute_cross_entropy(self, text):\n",
    "        encodings = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        input_ids = encodings['input_ids']\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "        return loss * input_ids.size(1)\n",
    "\n",
    "    def apply_cone(self, query_text, topk_indices):\n",
    "        candidate_scores = []\n",
    "        for idx in topk_indices:\n",
    "            demo = self.raw_texts[idx]\n",
    "            prompt_with_query = demo + \"\\n\" + query_text\n",
    "            H_xc = self.compute_cross_entropy(prompt_with_query)\n",
    "            H_c = self.compute_cross_entropy(demo)\n",
    "            H_cond = H_xc - H_c\n",
    "            candidate_scores.append((idx, H_cond))\n",
    "        sorted_indices = [idx for idx, _ in sorted(candidate_scores, key=lambda x: x[1])]\n",
    "        return sorted_indices[:self.k]\n",
    "\n",
    "    def select_demonstrations(self, query_embedding, query_text):\n",
    "        topk_indices = self.get_topk(query_embedding)\n",
    "        refined_indices = self.apply_cone(query_text, topk_indices)\n",
    "        return refined_indices\n",
    "\n",
    "# ----------------------\n",
    "# Dataset Loading Functions\n",
    "# ----------------------\n",
    "def load_commonsenseqa(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"commonsense_qa\", split=split)\n",
    "    dataset = dataset.shuffle().select(range(min(num_samples, len(dataset))))\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        question = example['question']\n",
    "        choices = example['choices']['text']\n",
    "        label_index = example['answerKey']\n",
    "        answer = example['choices']['label'].index(label_index)\n",
    "        choices_text = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(example['choices']['label'], choices)])\n",
    "        prompt = f\"Question: {question}\\n{choices_text}\"\n",
    "        label_text = example['choices']['text'][answer]\n",
    "        full_prompt = f\"{prompt}\\nAnswer: {label_text}\"\n",
    "        texts.append(full_prompt)\n",
    "        labels.append(label_text)\n",
    "    return texts, labels\n",
    "\n",
    "def load_ag_news(split=\"train\", num_samples=1000):\n",
    "    label_map = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
    "    dataset = load_dataset(\"sh0416/ag_news\", split=split)\n",
    "    dataset = dataset.shuffle().select(range(num_samples))\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        label = sample[\"label\"]\n",
    "        title = sample[\"title\"]\n",
    "        description = sample[\"description\"]\n",
    "        full_text = f\"Title: {title}\\nDescription: {description}\\nLabel: {label}\"\n",
    "        texts.append(full_text)\n",
    "        labels.append(str(label))\n",
    "    return texts, labels\n",
    "\n",
    "def load_sst5(split=\"train\", num_samples=1000):\n",
    "    dataset = load_dataset(\"SetFit/sst5\", \"default\", split=split)  \n",
    "    dataset = dataset.filter(lambda x: x[\"label_text\"] is not None)\n",
    "    dataset = dataset.shuffle().select(range(min(num_samples, len(dataset))))\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"].strip().replace(\"\\n\", \" \")\n",
    "        label_text = sample[\"label_text\"]\n",
    "        full_text = f\"Text: {text}\\nLabel: {label_text}\"\n",
    "        texts.append(full_text)\n",
    "        labels.append(label_text)\n",
    "    return texts, labels\n",
    "\n",
    "def load_dataset_by_name(name, num_samples=100):\n",
    "    if name == \"commonsenseqa\":\n",
    "        return load_commonsenseqa(num_samples=num_samples)\n",
    "    elif name == \"ag_news\":\n",
    "        return load_ag_news(num_samples=num_samples)\n",
    "    elif name == \"sst5\":\n",
    "        return load_sst5(num_samples=num_samples)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {name}\")\n",
    "\n",
    "# ----------------------\n",
    "# Embedding and Generation\n",
    "# ----------------------\n",
    "def embed_texts(texts, model_name='all-MiniLM-L6-v2', batch_size=64):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def generate_label_from_prompt(prompt, model, tokenizer, device):\n",
    "    enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_label = decoded.split(\"Answer:\" if \"Answer:\" in prompt else \"Label:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "    return generated_label\n",
    "\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "def evaluate_cone(texts, labels, embeddings, model_name, k=5, batch_size=3):\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    print(\"\\n[1] Starting batch evaluation (3 random queries)...\")\n",
    "    indices = random.sample(range(len(texts)), batch_size)\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    for idx in indices:\n",
    "        separator = \"Answer:\" if \"Answer:\" in texts[idx] else \"Label:\"\n",
    "        query_input = texts[idx].rsplit(f\"\\n{separator}\", 1)[0] + f\"\\n{separator}\"\n",
    "        query_label = labels[idx]\n",
    "        query_embedding = embeddings[idx]\n",
    "\n",
    "        candidate_embeddings = np.delete(embeddings, idx, axis=0)\n",
    "        candidate_texts = [t for i, t in enumerate(texts) if i != idx]\n",
    "        candidate_labels = [l for i, l in enumerate(labels) if i != idx]\n",
    "\n",
    "        selector = TopK_CoNE(candidate_embeddings, candidate_texts, k=k)\n",
    "        selector.model = model\n",
    "        selector.tokenizer = tokenizer\n",
    "        selector.device = device\n",
    "        demo_indices = selector.select_demonstrations(query_embedding, query_input)\n",
    "\n",
    "        demos = [candidate_texts[i] + f\"\\n{separator} {candidate_labels[i]}\" for i in demo_indices]\n",
    "        #print(\"len demos:\", len(demos))\n",
    "        prompt = \"\\n\\n\".join(demos) + f\"\\n\\n{query_input}\"\n",
    "        pred_label = generate_label_from_prompt(prompt, model, tokenizer, device)\n",
    "\n",
    "        accs.append(int(str(pred_label).strip().lower() == str(query_label).strip().lower()))\n",
    "        f1s.append(f1_score([str(query_label)], [str(pred_label)], average='macro'))\n",
    "\n",
    "        #print(\"pred label:\", pred_label)\n",
    "        #print(\"gold:\", query_label)\n",
    "\n",
    "    avg_acc = np.mean(accs)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    print(f\"\\n[2] Evaluation over {batch_size} samples\")\n",
    "    print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    return avg_acc, avg_f1\n",
    "\n",
    "# ----------------------\n",
    "# Main Driver\n",
    "# ----------------------\n",
    "def run_demo_selection(model_name='openai-community/gpt2', dataset_name='ag_news'):\n",
    "    print(f\"\\n[1] Loading and embedding {dataset_name} dataset...\")\n",
    "    texts, labels = load_dataset_by_name(dataset_name, num_samples=100)\n",
    "    embeddings = embed_texts(texts)\n",
    "    evaluate_cone(texts, labels, embeddings, k=8, batch_size=100, model_name=model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo_selection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c5bc9-f96a-4235-954f-0c36539fa788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
